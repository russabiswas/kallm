<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Workshop on Knowledge Graph and Large Language Models (KaLLM)</title>
        <!-- Favicon
        <link rel="icon" type="image/x-icon" href="favicon.ico" /> -->
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
            <div class="container">
          <!--      <a class="navbar-brand" href="#page-top"><img src="img/navbar-logo.svg" alt="..." /></a> -->
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars ms-1"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav text-uppercase ms-auto py-4 py-lg-0">
                        <li class="nav-item"><a class="nav-link" href="#home">Home</a></li>
                        <li class="nav-item"><a class="nav-link" href="#callforpapers">Call for Papers</a></li>
                        <li class="nav-item"><a class="nav-link" href="#program">Program</a></li>
                        <li class="nav-item"><a class="nav-link" href="#team">Keynote Speakers</a></li>
                        <li class="nav-item"><a class="nav-link" href="#organisers">Organising Committee</a></li>
                        <li class="nav-item"><a class="nav-link" href="#contact">Contact</a></li>
                   <!--     <li class="nav-item"><a class="nav-link" href="#contact">Contact</a></li> -->
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Masthead-->
       
            <div class="container">
               
                 <img src="img/intro.jpg" alt="..." class="coverpic">
        <!--        <a class="btn btn-primary btn-xl text-uppercase" href="#home">Home</a> -->
            </div>
       
        
        
        
   <!--      <header class="masthead">
            <div class="container">
                <div class="masthead-subheading"></div>
                <div class="masthead-heading text-uppercase"></div>
                 <img src="img/header.jpg" alt="..." class="coverpic">
        <!--        <a class="btn btn-primary btn-xl text-uppercase" href="#home">Home</a> 
            </div>
        </header>-->
        <!-- Home-->
        <section class="page-section" id="Home">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">Home</h2>
                     <img src="img/motive.jpg" alt="..." class="center">
                    
                <p>    This work focuses on predicting the entity types solely from their label names, e.g., <i> What is pangolin?</i>, or  <i> What is a violin?</i>. As humans we have the background knowledge that <i>pangolin is an animal</i> and <i>violin is an instrument</i>. There are many such entities in DBpedia as depicted in the figure above for which the entities are typed to the most general class <i>owl: Thing</i>. Neural Language Models (NLMs) are exploited to predict the types of the entities.     </p>           
              
                </div>
                </div>
            </div>
        </section>
        <!-- Call for Papers Grid-->
        <section class="page-section bg-light" id="Call for Papers">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">Language Models</h2>
              <!--        <h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3> -->
                    <blockquote cite="https://arxiv.org/abs/cs/0108005">
                   Language modeling is the art of determining the probability of a sequence of words. This is useful in a large variety of areas including speech recognition, optical character recognition, handwriting recognition, machine translation, and spelling correction.
                   </blockquote>
                    <p><a href="https://arxiv.org/abs/cs/0108005"> A Bit of Progress in Language Modeling</a></p>
        
                    <button type="button" class="collapsible"><b>Word2Vec</b></button>
                    <div class="content">
                  <img src="img/word2vec.jpg" alt="..." class="center">

                        <p> It  aims  to  learn  the  distributed  representation  for  words  reducing  the  high  dimensional  word  representations  in  a  large  corpus.  The  CBOW Word2Vec model predicts the current word from a window of context words and the skip-gram model predicts the context words based on the current word. In the figure, the model consists of a vocabulary of V words, a context of C words, a dense representation of N-dimensional word vector, an embedding matrix W of dimensions VxN at the input and a context matrix W' of dimensions NxV at the output.
                        
                        
                        </p>
                    </div>
                    <button type="button" class="collapsible"><b>GloVe</b></button>
                    <div class="content">
                        <img src="img/glove.jpg" alt="..." class="center">
                    <p>GloVe exploits the global word-word co-occurrence statistics in the corpus  with  the  underlying  intuition  that  the  ratios  of  word-word  co-occurrence probabilities encode some form of the meaning of the words. It is essentially a log-bilinear model with a weighted least-squares objective. 
The training objective of GloVe is to learn word vectors such that their dot product equals the logarithm of the words' probability of co-occurrence. Owing to the fact that the logarithm of a ratio equals the difference of logarithms, this objective associates (the logarithm of) ratios of co-occurrence probabilities with vector differences in the word vector space. These ratios encode some form of meaning of the words, hence, this information gets encoded as vector differences as well.                        
                        
                        
                        
                        </p>
                    </div>
                    <button type="button" class="collapsible"><b>Wikipedia2Vec</b></button>
                    <div class="content">
                        <img src="img/wiki2vec.jpg" alt="..." class="center">
                        <p>The model jointly learns word and entity embeddings from Wikipedia where similar words and entities are close to one another in the vector space.  It comprises of three submodels namely Wikipedia Link Graph Model, Word-based skip-gram model, and Anchor context model.
                        <ul>
                         <li>Wikipedia Link Graph Model comprises of an undirected link graph in which the nodes are the Wikipedia entities and there exists an edge between two nodes if the page of one entity links to the other or if both pages link each other. It learns entity embeddings by predicting neighboring entities in the graph.</li> 
                         <li>Word-based skip-gram model learns word embeddings by predicting neighboring words given each word in a text contained on a Wikipedia page.</li>
                         <li>Anchor context model learns embeddings by predicting neighboring words given each entity and aims to place similar words and entities near one another in the vector space </li>
                         </ul>                     
                        </p> 
                    </div>
                    <button type="button" class="collapsible"><b>BERT</b></button>
                    <div class="content">
                        <img src="img/bert.jpg" alt="..." class="center">
                        <p>Bidirectional Encoder Representations from Transformers is a contextual information based embedding approach in which pretraining on bidirectional representations from unlabeled text by using the left and the right context in all the layers is performed. BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT’s goal is to generate a language model, only the encoder mechanism is necessary. The encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, which allows the model to learn the context of a word based on all of its surroundings.                                     
                        </p>  </div>
                    <button type="button" class="collapsible"><b>Character Embedding</b></button>
                    <div class="content">
                     <img src="img/charembed.jpg" alt="..." class="center">
                    <p>Character embedding represents the latent representations of characters trained over a corpus which helps in determining the vector representations of out-of-vocabulary words. Character level embedding uses one-dimensional convolutional neural network (1D-CNN) to find numeric representation of words by looking at their character-level compositions.</p>
                    </div>          
                </div>
            
        </section>
        <!-- Program-->
        <section class="page-section" id="Program">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">Entity Typing</h2>
                  
                    <blockquote cite="#!">
                  Entity Typing in the task of predicting types of entities in a Knowledge Graph and it plays a vital role in Knowledge Graph construction and completion.
                   </blockquote>
                    
                    
                </div>
                
                <button type="button" class="collapsible"><b>Embeddings of the Entity Names</b></button>
                    <div class="content">
                   

                        <p> The embedding of the entity names are generated using  Neural Language Models (NLMs). To do so, pre-trained Word2Vec model on Google News dataset, GloVe model pre-trained on Wikipedia 2014 version and Gigaword 5, Wikipedia2Vec model pre-trained on English Wikipedia 2018 version, and pre-trained English character embeddings derived from GloVe 840B/300D dataset are used with a vector dimension of 300. The average of all word vectors in the entity names is taken as the vector representation of the entities. For BERT, the average of the last four hidden layers of the model is taken as a representation of the names of entities and the dimension used is 768.</p>
                    </div>
                
                
                <button type="button" class="collapsible"><b>Classification</b></button>
                    <div class="content">
                    <img src="img/classifier.jpg" alt="..." class="center">

                        <p> Two classifiers have been built on top of the NLMs: <ol>
                         <li>Fully Connected Neural Network (FCNN), and</li> 
                            <li>Convolutional Neural Network (CNN).</li> </ol> 
                            A three-layered FCNN model consisting of two dense layers with ReLU as an activation function has been used on the top of the vectors                             generated from the NLMs. The softmax function is used in the last layer to calculate the probability of the entities belonging to                                 different classes. The CNN model consists of two 1-D convolutional layers followed by a global max-pooling layer. ReLu is used as an                             activation function in the convolutional layers and the output of the pooling layer is then passed through a fully connected final                               layer, in which the softmax function predicts the classes of the entities.</p>
                    </div>
            
                <button type="button" class="collapsible"><b>Experiments</b></button>
                    <div class="content">    
                        <p>
                            <b>Datasets</b> <br>
                            The experiments are conducted on the benchmark dataset DBpedia630k extracted from DBpedia consisting of 7 non-overlapping classes with 560,000 train and 70,000 test entities. The classes are:
                            <ol>
                         <li>dbo: Person,</li> 
                         <li>dbo: Organisation,</li>
                         <li>dbo: MeansofTransportation,</li>
                         <li>dbo: Place,</li> 
                         <li>dbo: Animal,</li>
                         <li>dbo: Plant, and</li>
                         <li>dbo: Work.</li> </ol> 
                         Furthermore, to evaluate the approaches independently of DBpedia, an additional test set composed of entities from CaLiGraph. It is a Wikipedia-based KG containing entities extracted from tables and enumerations in Wikipedia articles. It consists of 70,000 entities that are unknown to DBpedia and are evenly distributed among 7 aforementioned classes.
                        </p>
                        
            <p><b>Results</b></p>
                        
                        
            <table class="tg">
<thead>
  <tr>
    <th class="tg-kt03" rowspan="2"><b>Embedding Models</b></th>
    <th class="tg-kt03" colspan="2"><b>Types in Labels</b></th>
    <th class="tg-kt03" colspan="2"><b>No Types in Labels</b></th>
    <th class="tg-kt03" colspan="2"><b>CaLiGraph Test set</b></th>
  </tr>
  <tr>
    <td class="tg-kt03"><b>FCNN</b></td>
    <td class="tg-kt03"><b>CNN</b></td>
    <td class="tg-kt03"><b>FCNN</b></td>
    <td class="tg-kt03"><b>CNN</b></td>
    <td class="tg-kt03"><b>FCNN</b></td>
    <td class="tg-kt03"><b>CNN</b></td>
  </tr>
</thead>
<tbody>
  <tr>
      <td class="tg-kt03"><b>Word2Vec</b></td>
    <td class="tg-kt03">80.11</td>
    <td class="tg-kt03">46.71</td>
    <td class="tg-kt03">72.08</td>
    <td class="tg-kt03">44.39</td>
    <td class="tg-kt03">48.93</td>
    <td class="tg-kt03">25.91</td>
  </tr>
  <tr>
      <td class="tg-kt03"><b>GloVe</b></td>
    <td class="tg-kt03">83.34</td>
    <td class="tg-kt03">54.06</td>
    <td class="tg-kt03">82.62</td>
    <td class="tg-kt03">53.41</td>
    <td class="tg-kt03">61.88</td>
    <td class="tg-kt03">31.3</td>
  </tr>
  <tr>
      <td class="tg-kt03"><b>Wikipedia2Vec</b></td>
    <td class="tg-kt03"><b>91.14</b></td>
    <td class="tg-kt03">60.47</td>
    <td class="tg-kt03"><b>90.68</b></td>
    <td class="tg-kt03">57.36</td>
    <td class="tg-kt03"><b>75.21</b></td>
    <td class="tg-kt03">36.97</td>
  </tr>
  <tr>
      <td class="tg-kt03"><b>BERT</b></td>
    <td class="tg-kt03">67.37</td>
    <td class="tg-kt03"><b>62.27</b></td>
    <td class="tg-kt03">64.63</td>
    <td class="tg-kt03"><b>60.4</b></td>
    <td class="tg-kt03">53.42</td>
    <td class="tg-kt03">35.55</td>
  </tr>
  <tr>
      <td class="tg-kt03"><b>Character Embedding</b></td>
    <td class="tg-kt03">73.43</td>
    <td class="tg-kt03">58.13</td>
    <td class="tg-kt03">72.66</td>
    <td class="tg-kt03">58.3</td>
    <td class="tg-kt03">54.91</td>
    <td class="tg-kt03"><b>45.73</b></td>
  </tr>
</tbody>
</table>
 </div>
            
            
        </section>
        <!-- Team-->
        <section class="page-section bg-light" id="team">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">Our Amazing Team</h2>
            <!--        <h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3> -->
                </div>
                <div class="row">
                    <div class="col-lg-4">
                        <div class="team-member">
                            <img class="mx-auto rounded-circle" src="img/russa_web.jpg" alt="..." />
                            <h4>Russa Biswas</h4>
                            <p class="text-muted">FIZ Karlsruhe Leibniz Institute for Information Infrastructure, Germany</p>
                            <a class="btn btn-dark btn-social mx-2" href="https://twitter.com/russa_biswas"><i class="fab fa-twitter"></i></a>
               
                        </div>
                    </div>
                    <div class="col-lg-4">
                        <div class="team-member">
                            <img class="mx-auto rounded-circle" src="img/radina_sofronova.jpg" alt="..." />
                            <h4>Radina Sofronova</h4>
                            <p class="text-muted">FIZ Karlsruhe Leibniz Institute for Information Infrastructure, Germany</p>
                            <a class="btn btn-dark btn-social mx-2" href="https://twitter.com/RadinaSofronova"><i class="fab fa-twitter"></i></a>
                           
                        </div>
                    </div>
                    <div class="col-lg-4">
                        <div class="team-member">
                            <img class="mx-auto rounded-circle" src="img/mehwish_alam.jpg" alt="..." />
                            <h4>Mehwish Alam</h4>
                            <p class="text-muted">FIZ Karlsruhe Leibniz Institute for Information Infrastructure, Germany</p>
                            <a class="btn btn-dark btn-social mx-2" href="https://twitter.com/em_alam"><i class="fab fa-twitter"></i></a>
                         </div>  
                        </div>
                        <div class="col-lg-4">
                        <div class="team-member">
                            <img class="mx-auto rounded-circle" src="img/nicolas_heist.jpg" alt="..." />
                            <h4>Nicolas Heist</h4>
                            <p class="text-muted">University of Mannheim, <br> Germany</p>
                           
                            <a class="btn btn-dark btn-social mx-2" href="https://twitter.com/n_heist"><i class="fab fa-twitter"></i></a>
                         </div>  
                        </div>
                            <div class="col-lg-4">
                        <div class="team-member">
                            <img class="mx-auto rounded-circle" src="img/heiko_paulheim.jpg" alt="..." />
                            <h4>Heiko Paulheim</h4>
                            <p class="text-muted">University of Mannheim, <br> Germany</p>
                        
                            <a class="btn btn-dark btn-social mx-2" href="https://twitter.com/heikopaulheim"><i class="fab fa-twitter"></i></a>
                           </div>
                        </div>
                                <div class="col-lg-4">
                                <div class="team-member">
                            <img class="mx-auto rounded-circle" src="img/harald_sack.jpg" alt="..." />
                            <h4>Harald Sack</h4>
                            <p class="text-muted">FIZ Karlsruhe Leibniz Institute for Information Infrastructure, Germany</p>
                            <a class="btn btn-dark btn-social mx-2" href="https://twitter.com/lysander07"><i class="fab fa-twitter"></i></a>
                           </div>
                         </div>
                    
                        </div>
                    </div>
            </div>
            </div>
        </section>
        <section class="page-section bg-light" id="team">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">How to cite the paper</h2>
                     <div class="how-to-cite">
                     <p> @inproceedings{biswas2021judge, <br>
                    title={Do Judge an Entity by its Name! Entity Typing using Language Models}, <br>
                    author={Biswas, Russa and Sofronova, Radina and Alam, Mehwish and Heist, Nicolas and Paulheim, Heiko and Sack, Harald}, <br>
                    booktitle={18th Extended Semantic Web Conference (ESWC), Poster and Demo Track}, <br>
                    year={2021}, <br>
                    organization={Springer}}
            </p>
            <!--        <h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3> -->
                </div>
                 </div>
                </div>
                </section>


      
        <!-- Clients-->
        <div class="py-5">
            <div class="container">
                <div class="row align-items-center">
                     <img src="img/logo.jpg" alt="..." class="center">
                    
                </div>
            </div>
        </div>
      
        <!-- Footer-->
        <footer class="footer py-4">
            <div class="container">
                <div class="row align-items-center">
                    <div>
                       <p> Copyright &copy; This website is maintained by Russa Biswas. For further questions, please contact <a href = "mailto: russa.biswas@fiz-karlsruhe.de">Email</a> 
                        
                        <!-- This script automatically adds the current year to your website footer-->
                        <!-- (credit: https://updateyourfooter.com/)-->
                        <script>
                            document.write(new Date().getFullYear());
                        </script> 
                           </p>
                    </div>
                </div>
            </div>
        </footer>
    
    
    
     <script>
            var coll = document.getElementsByClassName("collapsible");
            var i;

            for (i = 0; i < coll.length; i++) {
                coll[i].addEventListener("click", function() {
                    this.classList.toggle("active");
                    var content = this.nextElementSibling;
                    if (content.style.display === "block") {
                        content.style.display = "none";
                    } else {
                        content.style.display = "block";
                    }
                });
              }
            </script>
                    
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
